{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31f572e8-1769-48cf-94b0-5473e2748190",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Mount Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11afa8f6-0a4e-42ff-a0ee-d4faa32313b4",
     "showTitle": false,
     "title": "Mount Bucket"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "\n",
    "aws_keys_df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"sep\", \",\")\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")\n",
    "\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "\n",
    "AWS_S3_BUCKET = \"user-0ea903d23769-bucket\"\n",
    "MOUNT_NAME = \"/mnt/0ea903d23769-bucket\"\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5283f44-f48f-4c5a-928b-0adba84e6f17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read Topic Contents Into Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1854e059-732b-4acf-8a17-380906581211",
     "showTitle": false,
     "title": "Read Topic Contents Into Dataframes"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(spark, file_type, infer_schema, location):\n",
    "    return spark.read.format(file_type) \\\n",
    "                    .option(\"inferSchema\", infer_schema) \\\n",
    "                    .load(location)\n",
    "\n",
    "file_type = \"json\"\n",
    "infer_schema = \"true\"\n",
    "\n",
    "location_pin = \"/mnt/0ea903d23769-bucket/topics/0ea903d23769.pin/partition=0/*.json\"\n",
    "location_geo = \"/mnt/0ea903d23769-bucket/topics/0ea903d23769.geo/partition=0/*.json\"\n",
    "location_user = \"/mnt/0ea903d23769-bucket/topics/0ea903d23769.user/partition=0/*.json\"\n",
    "\n",
    "df_pin_dirty = read_data(spark, file_type, infer_schema, location_pin)\n",
    "df_geo_dirty = read_data(spark, file_type, infer_schema, location_geo)\n",
    "df_user_dirty = read_data(spark, file_type, infer_schema, location_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e3bd8f5-dff0-4517-b529-974158fc66a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Download Dirty Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eaa77b9-6955-49af-91c1-8b10e70b1dd8",
     "showTitle": false,
     "title": "Download Dirty CSVs"
    }
   },
   "outputs": [],
   "source": [
    "display(df_pin_dirty)\n",
    "display(df_geo_dirty)\n",
    "display(df_use_dirty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e58aa76a-4664-4808-9144-fd64672f3f50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6ccb954-4d5a-434a-89a5-088f88fdaaae",
     "showTitle": false,
     "title": "Create Spark Session"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, TimestampType, DateType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DF Cleaning\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf5fb4c4-0385-4e0b-b63d-0f0ada5710f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clean df_pin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d1c552c-e42e-4f84-9a57-ca0a31934012",
     "showTitle": false,
     "title": "Cleaning df_pin"
    }
   },
   "outputs": [],
   "source": [
    "df_pin = df_pin_dirty\n",
    "\n",
    "# Replace empty entries and entries with no relevant data in each column with Nones\n",
    "df_pin = df_pin.replace(['', ' ', 'NULL', 'null'], [None] * 4)\n",
    "df_pin = df_pin.withColumn(\"description\", when(col(\"description\") == \"No description available Story format\", None).otherwise(col(\"description\")))\n",
    "df_pin = df_pin.withColumn(\"follower_count\", when(col(\"follower_count\") == \"User Info Error\", None).otherwise(col(\"follower_count\")))\n",
    "df_pin = df_pin.withColumn(\"image_src\", when(col(\"image_src\") == \"Image src error.\", None).otherwise(col(\"image_src\")))\n",
    "df_pin = df_pin.withColumn(\"poster_name\", when(col(\"poster_name\") == \"User Info Error\", None).otherwise(col(\"poster_name\")))\n",
    "df_pin = df_pin.withColumn(\"tag_list\", when(col(\"tag_list\") == \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\", None).otherwise(col(\"tag_list\")))\n",
    "df_pin = df_pin.withColumn(\"title\", when(col(\"title\") == \"No Title Data Available\", None).otherwise(col(\"title\")))\n",
    "\n",
    "# Transform follower_count to ensure every entry is a number and data type is an int\n",
    "# Remove any non-numeric characters (like 'k' in '136k') and then convert to integer\n",
    "df_pin = df_pin.withColumn(\"follower_count\", regexp_replace(col(\"follower_count\"), \"[^0-9]\", \"\"))\n",
    "df_pin = df_pin.withColumn(\"follower_count\", col(\"follower_count\").cast(IntegerType()))\n",
    "\n",
    "# Ensure that each column containing numeric data has a numeric data type\n",
    "df_pin = df_pin.withColumn(\"downloaded\", col(\"downloaded\").cast(IntegerType()))\n",
    "\n",
    "# Clean the data in the save_location column to include only the save location path\n",
    "df_pin = df_pin.withColumn(\"save_location\", regexp_replace(col(\"save_location\"), \"Local save in \", \"\"))\n",
    "\n",
    "# Rename the index column to ind\n",
    "df_pin = df_pin.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df_pin = df_pin.select([\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\"])\n",
    "\n",
    "\n",
    "display(df_pin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2cd2c50-5f1b-4c54-b77e-7dba1f01ffff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clean df_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6baa796d-787f-4eaf-9978-405aaab532c4",
     "showTitle": false,
     "title": "Cleaning df_geo"
    }
   },
   "outputs": [],
   "source": [
    "df_geo = df_geo_dirty\n",
    "\n",
    "# Create a new column 'coordinates' containing an array of latitude and longitude\n",
    "df_geo = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "\n",
    "# Drop the latitude and longitude columns\n",
    "df_geo = df_geo.drop(\"latitude\", \"longitude\")\n",
    "\n",
    "# Convert the timestamp column from a string to a timestamp data type\n",
    "df_geo = df_geo.withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df_geo = df_geo.select([\"ind\", \"country\", \"coordinates\", \"timestamp\"])\n",
    "\n",
    "display(df_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c28900b-b110-45de-aa2a-adedecc08850",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clean df_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06243b93-0ed4-4bac-b9b9-bc2c6b35aa16",
     "showTitle": false,
     "title": "Cleaning df_user"
    }
   },
   "outputs": [],
   "source": [
    "df_user = df_user_dirty\n",
    "\n",
    "# Create a new column 'user_name' by concatenating 'first_name' and 'last_name'\n",
    "df_user = df_user.withColumn(\"user_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "\n",
    "# Drop the 'first_name' and 'last_name' columns\n",
    "df_user = df_user.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "# Convert the 'date_joined' column from a string to a timestamp data type\n",
    "df_user = df_user.withColumn(\"date_joined\", col(\"date_joined\").cast(TimestampType()))\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df_user = df_user.select([\"ind\", \"user_name\", \"age\", \"date_joined\"])\n",
    "\n",
    "display(df_user)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Process_Batch_Data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
